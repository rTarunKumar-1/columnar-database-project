# import json
# import torch
# import torch.nn as nn
# from torch.utils.data import TensorDataset, DataLoader
# from model import LSTMPrefetcher


# def main():
#     # load training dataset generated by training_set_generator.py
#     with open("training_dataset.json", "r") as f:
#         data = json.load(f)

#     # inputs and labels are already sequences of indices
#     X = torch.tensor(data["inputs"], dtype=torch.long)  # [N, window]
#     Y = torch.tensor(data["labels"], dtype=torch.long)  # [N]

#     # mappings from original block ids to indices and back
#     id2idx = {int(k): int(v) for k, v in data["id2idx"].items()}
#     idx2id = {int(k): int(v) for k, v in data["idx2id"].items()}
#     vocab_size = int(data["vocab_size"])

#     print(f"loaded dataset with {X.shape[0]} samples")
#     print(f"window size: {X.shape[1]}")
#     print(f"vocab size: {vocab_size}")

#     # build model using the same definition as in model.py
#     # num_tokens must be the embedding vocabulary size
#     model = LSTMPrefetcher(
#         num_tokens=vocab_size,
#         embed_dim=16,
#         hidden_dim=64,
#         num_layers=1,
#     )

#     criterion = nn.CrossEntropyLoss()
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

#     dataset = TensorDataset(X, Y)
#     loader = DataLoader(dataset, batch_size=32, shuffle=True)

#     window = X.shape[1]

#     epochs = 10
#     for epoch in range(epochs):
#         model.train()
#         total_loss = 0.0

#         for batch_x, batch_y in loader:
#             # batch_x shape [B, window]
#             lengths = torch.full(
#                 (batch_x.size(0),),
#                 window,
#                 dtype=torch.long,
#             )

#             optimizer.zero_grad()
#             logits = model(batch_x, lengths)
#             loss = criterion(logits, batch_y)
#             loss.backward()
#             optimizer.step()

#             total_loss += loss.item()

#         print(f"epoch {epoch + 1} - loss {total_loss:.4f}")

#     # save model weights
#     torch.save(model.state_dict(), "trained_model.pt")

#     # save mapping so scheduler can use the same id space
#     with open("trained_mappings.json", "w") as f:
#         json.dump(
#             {
#                 "id2idx": id2idx,
#                 "idx2id": idx2id,
#             },
#             f,
#             indent=2,
#         )

#     print("training complete - saved trained_model.pt and trained_mappings.json")


# if __name__ == "__main__":
#     main()

import json
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from model import LSTMPrefetcher

def main():
    # Load training dataset
    with open("training_dataset.json", "r") as f:
        data = json.load(f)
    
    X = torch.tensor(data["inputs"], dtype=torch.long)  # [N, window]
    Y = torch.tensor(data["labels"], dtype=torch.long)  # [N]
    
    id2idx = {int(k): int(v) for k, v in data["id2idx"].items()}
    idx2id = {int(k): int(v) for k, v in data["idx2id"].items()}
    vocab_size = int(data["vocab_size"])
    
    print(f"loaded dataset with {X.shape[0]} samples")
    print(f"window size: {X.shape[1]}")
    print(f"vocab size: {vocab_size}")
    
    # Build model
    model = LSTMPrefetcher(
        num_tokens=vocab_size,
        embed_dim=16,
        hidden_dim=64,
        num_layers=1,
    )
    
    # ✅ Convert labels to multi-hot encoding for BCE
    Y_multihot = torch.zeros(Y.size(0), vocab_size, dtype=torch.float)
    Y_multihot[torch.arange(Y.size(0)), Y] = 1.0
    
    # ✅ Use BCEWithLogitsLoss for multi-label sigmoid
    criterion = nn.BCEWithLogitsLoss()
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    dataset = TensorDataset(X, Y_multihot)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)
    window = X.shape[1]
    epochs = 10
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        for batch_x, batch_y in loader:
            lengths = torch.full(
                (batch_x.size(0),),
                window,
                dtype=torch.long,
            )
            
            optimizer.zero_grad()
            logits = model(batch_x, lengths)
            loss = criterion(logits, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        print(f"epoch {epoch + 1} - loss {total_loss:.4f}")
    
    # Save model
    torch.save(model.state_dict(), "trained_model.pt")
    
    # ✅ FIX: Save vocab_size in mappings
    with open("trained_mappings.json", "w") as f:
        json.dump(
            {
                "id2idx": id2idx,
                "idx2id": idx2id,
                "vocab_size": vocab_size  # ✅ Added
            },
            f,
            indent=2,
        )
    print("training complete - saved trained_model.pt and trained_mappings.json")

if __name__ == "__main__":
    main()
